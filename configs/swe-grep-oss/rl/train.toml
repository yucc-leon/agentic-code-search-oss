max_steps = 150

[model]
name = "willcb/Qwen3-8B"

[model.ac]
freq = 1

[model.experimental.lora]
rank = 64
alpha = 512
dropout = 0.0
target_modules = [
    "q_proj",      # Attention: Query projection
    "k_proj",      # Attention: Key projection
    "v_proj",      # Attention: Value projection
    "o_proj",      # Attention: Output projection
    "gate_proj",   # MLP: Gating projection
    "up_proj",     # MLP: Up projection
    "down_proj"    # MLP: Down projection
]
modules_to_save = []

[optim]
lr = 1e-5

[ckpt]
interval = 10